import json
import logging
from collections.abc import Mapping
from pathlib import Path
from textwrap import dedent
from typing import Any, Literal

import boto3
import pandas as pd
import spacy
from botocore.config import Config
from botocore.exceptions import ReadTimeoutError
from dotenv import load_dotenv
from haystack import Document, Pipeline
from haystack.components.embedders import SentenceTransformersDocumentEmbedder
from haystack.components.writers import DocumentWriter
from haystack.document_stores.in_memory import InMemoryDocumentStore
from pydantic import BaseModel
from tqdm import tqdm

from src.components.agents.helpers import get_boto3_creds

from ...config.types import Config as QuarkAssistantConfig
from ..utils import Utils
from .gremlin import Gremlin

load_dotenv(override=True)


class PipelineResponse(BaseModel):
    """The response generated by the pipeline."""

    llm_enhanced_prompt: str
    generated_query: str
    db_response: str
    nl_db_response: str


class QueryEnhancerPipeline:
    """The query-enhancement pipeline."""

    def __init__(self, schema: dict[str, Any], config: QuarkAssistantConfig):
        self.document_store = InMemoryDocumentStore()
        document_content = json.dumps(schema, indent=2)
        document = Document(content=document_content)

        # Index the schema into the in-memory document store, so the RAG pipeline can
        # retrieve it later.
        indexing_pipeline = Pipeline()
        indexing_pipeline.add_component(
            instance=SentenceTransformersDocumentEmbedder(
                model="sentence-transformers/all-MiniLM-L6-v2"
            ),
            name="doc_embedder",
        )
        indexing_pipeline.add_component(
            instance=DocumentWriter(document_store=self.document_store),
            name="doc_writer",
        )
        indexing_pipeline.connect("doc_embedder.documents", "doc_writer.documents")

        # Run the indexing pipeline on the schema.
        indexing_pipeline.run({"doc_embedder": {"documents": [document]}})

        schema_context = document.content
        if schema_context is None:
            raise Exception("Couldn't derive schema context from file")
        self.schema_context = schema_context

        # Load gene alias mapping
        self.gene_alias_mapping = {}
        try:
            alias_mapping_path = Path(
                "datastores/neptune/gtex/data/gene_alias_mapping.json"
            )
            if alias_mapping_path.exists():
                with open(alias_mapping_path) as f:
                    self.gene_alias_mapping = json.load(f)
                logging.info(
                    f"Loaded {len(self.gene_alias_mapping)} gene alias mappings"
                )
            else:
                logging.warning("Gene alias mapping file not found")
        except Exception as e:
            logging.error(f"Failed to load gene alias mapping: {e}")

        # Use cross-account permissions if needed.
        client_kwargs = {
            "service_name": "bedrock-runtime",
            "region_name": "ap-southeast-1",
            "config": Config(retries={"max_attempts": 0}),
        }
        if config is not None:
            client_kwargs.update(get_boto3_creds(config.models.get("aws", {})))
        self.client = boto3.client(**client_kwargs)
        self.db_entity_type_mapper = {"gene_id": "gene"}
        self.graph_db_client = Gremlin(
            schema_context=schema_context,
            document_store=self.document_store,
            config=config,
        )

    def _resolve_gene_aliases(
        self, entities_to_types: Mapping[str, str]
    ) -> Mapping[str, str]:
        """Resolve gene aliases to official gene symbols with multi-level resolution.

        Parameters
        ----------
        entities_to_types: Mapping[str, str]
            The mapping of entities to their types.

        Returns
        -------
        resolved_entities_to_types: Mapping[str, str]
            The mapping with gene aliases resolved to official symbols.
        """
        resolved_entities = {}

        for entity, entity_type in entities_to_types.items():
            if entity_type == "gene" and entity in self.gene_alias_mapping:
                # Multi-level alias resolution
                current = entity
                visited = set()  # Prevent infinite loops
                resolution_chain = [current]

                while current in self.gene_alias_mapping and current not in visited:
                    visited.add(current)
                    next_symbol = self.gene_alias_mapping[current]

                    if next_symbol == current:
                        # Found the official symbol
                        break

                    current = next_symbol
                    resolution_chain.append(current)

                    # Safety check for infinite loops
                    if len(resolution_chain) > 10:
                        logging.warning(
                            f"Long alias chain detected: {' → '.join(resolution_chain)}"
                        )
                        break

                official_symbol = current
                resolved_entities[official_symbol] = entity_type

                if entity != official_symbol:
                    if len(resolution_chain) > 2:
                        logging.info(
                            f"Multi-level alias resolution: {' → '.join(resolution_chain)}"
                        )
                    else:
                        logging.info(
                            f"Resolved gene alias: {entity} → {official_symbol}"
                        )
            else:
                resolved_entities[entity] = entity_type

        return resolved_entities

    def _run_ner(self, prompt: str, model_path: Path) -> tuple[str, ...]:
        """Run the NER step.

        Parameters
        ----------
        prompt: str
            The text to be analyzed.
        model_path: pathlib.Path
            The path to the NER model.

        Returns
        -------
        entities: tuple[str, ...]
            The entities recognized.
        """
        logging.debug("Performing NER...")
        nlp = spacy.load(model_path)
        logging.debug("NER done")
        return tuple(ent.text for ent in nlp(prompt).ents)

    def _get_entity_types_from_txts(
        self, entities: tuple[str, ...], txts_path: Path
    ) -> Mapping[str, str]:
        """Get the entitytypes from the TXT files that contain the data.

        Pre-requisite steps: `_run_ner`.

        The TXT files indicate the entity types by file name, with one entity per line
        in the files. If the name and ID are both present for an entity, the ID is
        appended in parentheses.

        Parameters
        ----------
        entities: tuple[str, ...]
            The entities recognized by the NER step.
        txts_path: pathlib.Path
            The path to the directory that contains the TXT files.

        Returns
        -------
        entities_to_types: Mapping[str, str]
            The mapping of the entities to their types.
        """
        logging.debug("Deriving entity types from TXTs...")
        entities_to_types = {}
        checked_entities = []
        for txt_path in tqdm(txts_path.glob("*.txt")):
            entity_type = txt_path.stem
            entities_of_type = []
            with open(txt_path) as f:
                for line in f.read().splitlines():
                    if "(" in line and ")" in line:
                        line_entities = [
                            line.split("(")[0].strip(),
                            line.split("(")[1].split(")")[0].strip(),
                        ]
                        entities_of_type += line_entities
                    else:
                        entities_of_type.append(line.strip())

            for entity in entities:
                if entity in checked_entities:
                    continue
                entity_idx = Utils.get_index_of_string_in_list_case_insensitive(
                    entity, entities_of_type
                )
                if entity_idx >= 0:
                    entities_to_types[entities_of_type[entity_idx]] = (
                        self.db_entity_type_mapper.get(entity_type, entity_type)
                    )

        logging.debug("Derived entity types from TXTs")
        return entities_to_types

    def _get_entity_types_from_csvs(
        self, entities: tuple[str, ...], csvs_path: Path
    ) -> Mapping[str, str]:
        """Get the entity types from the CSV files that contain the data.

        Pre-requisite steps: `_run_ner`.

        Parameters
        ----------
        entities: tuple[str, ...]
            The entities recognized by the NER step.
        csvs_path: pathlib.Path
            The path to the directory that contains the CSV files.

        Returns
        -------
        entities_to_types: Mapping[str, str]
            The mapping of the entities to their types.
        """
        logging.debug("Deriving entity types from CSVs...")
        entities_to_types = {}
        checked_entities = []
        for csv_path in tqdm(csvs_path.glob("*.csv")):
            df = pd.read_csv(csv_path)
            for entity in entities:
                if entity in checked_entities:
                    continue
                info = df[df["id:String"].str.fullmatch(entity, case=False)]
                if len(info) > 0:
                    entity_type = info["preferred_id:String"].values[0]
                    entities_to_types[info["id:String"].values[0]] = (
                        self.db_entity_type_mapper.get(entity_type, entity_type)
                    )
        logging.debug("Derived entity types from CSVs")
        return entities_to_types

    def _make_llm_call(
        self,
        prompt: str,
        system_instructions: str,
        llm: str = "us.anthropic.claude-3-5-sonnet-20241022-v2:0",
        temperature: float = 0.0,
    ) -> str:
        """Make a text-to-text LLM call using boto3 and Amazon Bedrock."""
        logging.debug("Making an LLM call...")
        response = self.client.converse(
            modelId=llm,
            messages=[{"role": "user", "content": [{"text": prompt}]}],
            system=[{"text": system_instructions}],
            inferenceConfig={"temperature": temperature},
        )
        logging.debug("Made the LLM call")
        if (
            len(response["output"]["message"]["content"]) > 0
            and "text" in response["output"]["message"]["content"][0]
        ):
            return response["output"]["message"]["content"][0]["text"]
        raise Exception("LLM did not return a response")

    def _enhance_prompt_with_entities(
        self,
        original_prompt: str,
        entities_to_types: Mapping[str, str],
        llm: str = "us.anthropic.claude-3-5-sonnet-20241022-v2:0",
    ) -> str:
        is_enrichment_analysis = any(
            term in original_prompt.lower()
            for term in [
                "enrichment",
                "pathway analysis",
                "drug interaction",
                "disease association",
                "enrichment analysis",
            ]
        )

        """Enhance the original prompt.

        Pre-requisite steps: `_run_ner`; `_get_entity_types_from_csvs`.

        Enhancing steps taken:
        1. add information about the entities, if available.

        Parameters
        ----------
        original_prompt: str
            The text to be enhanced.
        entities_to_types: Mapping[str, str]
            The mapping of the entities to their types.
        llm: str, default="us.anthropic.claude-3-5-sonnet-20241022-v2:0"
            The LLM ID to use.

        Returns
        -------
        enhanced_prompt: str
            The prompt, enhanced.
        """
        logging.debug("Enhancing prompt with entities...")
        system_prompt = dedent(
            """
            The user will provide you some text and two JSON objects. The text will
            contain a question from a bioinformatician. The first JSON object, labeled
            "Entities," will contain information about certain biological entities found
            in the text and their types. The second JSON object, labeled "Schema," will
            contain the schema information about the bioinformatician's database. Your
            job is to enhance the text in the following ways. If the "Entities" JSON is
            empty, no entities need to be modified in the enhanced text. Just work with
            the schema in that case.

            1. Insert information about the entity types. For example, turn the text,
            "how many patients had the or4g4p expression?" with the entity information
            `{"OR4G4P": "gene"}` into, "How many patients had the gene OR4G4P
            expression?"
            2. Replace the term "expression," if present in relation with a gene, with
            the term "read-count expression."
            3. If there are any "expressions," i.e., "read-count expressions," ensure
            that any number associated with them is represented using numerical
            representation and in between single quotes. For example, turn, "or4g4p
            expression greater than or equal to zero," into "where the read-count
            expression of the gene OR4G4P is greater than or equal to '0'." Note that if
            these numbers are not directly related to the read-count expressions, this
            change should not be made.
            4. Check for any mention of the terms "sample," "tissue sample," or "tissue
            type." If these terms occur in the form, "kidney sample," replace those
            occurrences with the form, "where the tissue type is kidney." For example,
            turn, "which kidney samples..." into, "What are the sample IDs where the
            tissue type is kidney..."
            5. Convert specifications of sex into "where" form, e.g., turn, "female
            patients," into, "where the patients are female."
            6. Use the term "age group" for age ranges.
            7. Fix the grammar in the question, including capitalization, punctutation,
            and tense.
            8. If the text contains a request for a certain number of samples, patients,
            etc., ensure that:
                a. that information is clear in your response; and
                b. that number is not turned into a string.
            9. Convert terms into property names based on the given schema, e.g., if the
            schema defines the property "SMATSSCR," with a description that explains
            that this property contains the autolysis score, convert the term,
            "autolysis score" into "SMATSSCR" in the enhanced text.
            10. Make sure that any values in the original text are converted into the
            required data type based on the schema in the enhanced text.
            11. Only respond with the enhanced text, with no additional information,
            introduction, preamble, or post-amble.
            12. For enrichment analysis requests, ensure the response covers:
                - Disease associations (via StudyToDiseaseAssociation)
                - Drug interactions (via DrugToGeneAssociation) 
                - Pathway involvement (via Gene_pathway_association)
                - Clinical trial connections (via ClinicalEntity relationships)
            13. Structure enrichment responses to provide summaries for each category.

            """
        ).strip()
        llm_prompt = (
            dedent(
                """
                Please enhance the following text based on the JSON object of biological
                entities and their types and the schema.

                Text: <original_prompt>

                Entities:
                ```json
                <entities_to_types>
                ```

                Schema:
                ```json
                <schema>
                ```
                """
            )
            .replace("<original_prompt>", original_prompt)
            .replace("<entities_to_types>", json.dumps(entities_to_types, indent=2))
            .replace("<schema>", self.schema_context)
            .strip()
        )
        if is_enrichment_analysis:
            llm_prompt += "\n\nThis is an enrichment analysis request. Ensure the enhanced text asks for comprehensive disease, pathway, drug, and cellular location associations."

        logging.debug("Prompt given to LLM: %s", llm_prompt)
        return self._make_llm_call(
            prompt=llm_prompt, system_instructions=system_prompt, llm=llm
        )

    def _generate_query(self, question: str) -> str:
        """Generate the query for the graph DB.

        Pre-requisite steps: `_run_ner`; `_get_entity_types_from_csvs`;
        `_enhance_prompt_with_entities`.
        """
        return self.graph_db_client.generate_query(question)

    def _get_db_response(self, query: str) -> str | dict[str, Any]:
        """Generate the response from the graph DB.

        Pre-requisite steps: `_run_ner`; `_get_entity_types_from_csvs`;
        `_enhance_prompt_with_entities`; `_generate_query`.
        """
        return self.graph_db_client.get_db_response(query)

    def _db_response_to_natural_language(
        self,
        enhanced_prompt: str,
        db_response: str,
        llm: str = "us.anthropic.claude-3-5-sonnet-20241022-v2:0",
    ) -> str:
        """Convert the graph DB response to a natural language answer.

        Pre-requisite steps: `_run_ner`; `_get_entity_types_from_csvs`;
        `_enhance_prompt_with_entities`; `_generate_query`; `_get_db_response`.
        """
        logging.debug("Converting the DB response to natural language...")
        system_prompt = dedent(
            """
            Convert the graph database response into a clear, structured natural language answer.
            
            For enrichment analysis: organize by Disease, Pathway, Drug, Cellular Location, Literature Support.
            For other queries: organize logically based on the question.
            
            Include PMIDs as "PMID: [ID]" when present. Provide clinical relevance where applicable.
            Extract and present all relevant information from the database response.
            """
        ).strip()

        llm_prompt = (
            dedent(
                """
                Please convert the following Gremlin response into natural language, using the question that was originally asked and the graph database's schema as context.

                Question: <enhanced_prompt>

                Schema:
                ```json
                <schema>
                ```

                Graph DB response:
                <db_response>
                """
            )
            .replace("<enhanced_prompt>", enhanced_prompt)
            .replace("<schema>", self.schema_context)
            .replace("<db_response>", db_response)
            .strip()
        )
        logging.debug("Prompt for converting to natural language: %s", llm_prompt)
        return self._make_llm_call(
            prompt=llm_prompt, system_instructions=system_prompt, llm=llm
        )

    def run(
        self,
        prompt: str,
        model_path: Path,
        entities_path: Path,
        llm: str = "us.anthropic.claude-3-5-sonnet-20241022-v2:0",
        entity_file_type: Literal["txt", "csv"] = "csv",
    ) -> PipelineResponse:
        """Run the entire pipeline.

        The pipeline:
        1. run NER;
        2. check if any of the recognized entities can be found in the DB;
        3. enhance the original prompt with information about the entities found, if
          available;
        4. generate the query for the graph DB;
        5. get the response from the graph DB;
        6. convert the graph DB response into natural language; and
        7. output the natural-language version of the graph DB response.

        Parameters
        ----------
        prompt: str
            The text to be analyzed.
        model_path: pathlib.Path
            The path to the NER model.
        entities_path: pathlib.Path
            The path to the directory that contains the entity-information files.
        llm: str, default="us.anthropic.claude-3-5-sonnet-20241022-v2:0"
            The LLM ID to use.
        entities_file_type: Literal["csv", "txt"], default="csv"
            Whether the entities are in the CSV format or the TXT format.

        Returns
        -------
        response: str
            The LLM-enhanced prompt, generated query, and response from the DB.
        """
        entities_to_types = {}
        if entity_file_type == "csv":
            entities_to_types = self._get_entity_types_from_csvs(
                entities=self._run_ner(prompt=prompt, model_path=model_path),
                csvs_path=entities_path,
            )
        elif entity_file_type == "txt":
            entities_to_types = self._get_entity_types_from_txts(
                entities=self._run_ner(prompt=prompt, model_path=model_path),
                txts_path=entities_path,
            )
        else:
            raise NotImplementedError("Entity files can only be CSV or TXT")

        # Resolve gene aliases to official symbols
        entities_to_types = self._resolve_gene_aliases(entities_to_types)

        llm_enhanced_prompt = self._enhance_prompt_with_entities(
            original_prompt=prompt, entities_to_types=entities_to_types, llm=llm
        )
        generated_query = self._generate_query(question=llm_enhanced_prompt)
        db_response = ""
        try:
            db_response = self._get_db_response(query=generated_query)
            if isinstance(db_response, dict):
                db_response = (
                    db_response.get("result", {}).get("data", {}).get("@value")
                )
            db_response = "```json\n" + json.dumps(db_response, indent=2) + "\n```"
        except self.graph_db_client.neptune_client.exceptions.MalformedQueryException:
            db_response = "⚠️ Malformed query"
            nl_db_response = "⚠️ Malformed query"
        except ReadTimeoutError:
            db_response = "⚠️ Read time out"
            nl_db_response = "⚠️ Read time out"

        nl_db_response = self._db_response_to_natural_language(
            enhanced_prompt=llm_enhanced_prompt, db_response=db_response
        )
        logging.debug("LLM-enhanced prompt: %s", llm_enhanced_prompt)
        logging.debug("Generated query: %s", generated_query)
        logging.debug("DB response: %s", db_response)
        return PipelineResponse(
            llm_enhanced_prompt=llm_enhanced_prompt,
            generated_query=generated_query,
            db_response=db_response,
            nl_db_response=nl_db_response,
        )
